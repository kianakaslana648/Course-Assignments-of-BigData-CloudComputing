{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab: DataFrame API and Spark SQL\n",
    "\n",
    "\n",
    "## Social characteristics of the Marvel Universe\n",
    "\n",
    "In this lab, you be working with a dataset that was created about the Marvel Comics universe. The source data is a text file that was created with a graph analysis library outside of Spark, so the text file contains a lot of information and is not in a format that is easy to query with SQL. Do not worry that it mentions the word \"graph\", you can think of these tables like regular SQL tables. You are going to use the file in this lab to practice data ingestion, manipulation, and analysis using both the DataFrame API and Spark SQL. You can see more about the source data [here](http://bioinfo.uib.es/~joemiro/marvel.html).\n",
    "\n",
    "## Understanding the data file\n",
    "\n",
    "As mentioned above, the data file contains three different tables worth of info:\n",
    "\n",
    "- data on Marvel characters\n",
    "- data on Marvel publications\n",
    "- data on which character appeared in which publication\n",
    "\n",
    "This data is not yet separated into three different tables. It is all combined into a single file. You will use PySparkSQL to extract the information needed. Each character has its own **primary key**, and each publication has its own **primary key**. The third table will have the connections between character **primary keys** and publication **primary keys**. \n",
    "\n",
    "Eventually you will combine all three tables together into an analytics dataset, where you will answer the following questions:\n",
    "\n",
    "- What are the top 10 publications with the most characters in them?\n",
    "- What are the top 10 characters who appear in the most publications?\n",
    "\n",
    "### The details of the data file are critical to extracting the data properly. The file is separated into two sections:\n",
    "\n",
    "#### The characters/publications (aka vertices or nodes) (section begins in line 1 with a header in the form of `*Vertices 19428 6486`):\n",
    "\n",
    "- Characters: lines 2-6487 in the format `1 \"24-HOUR MAN/EMMANUEL\"`, where `1` is the character primary key (aka node id) and the name of the character is within double quotes. The character table should look like the table sample below after processing:\n",
    "\n",
    "|character_id|           character|\n",
    "|------------|--------------------|\n",
    "|           1|24-HOUR MAN/EMMANUEL|\n",
    "|           2|3-D MAN/CHARLES CHAN|\n",
    "\n",
    "- Publications: lines 6488-19429 in the same format as characters like `6487 \"AA2 35\"` with the publication primary key and the name of the publication. The publciation table should look like the table sample below after processing.\n",
    "\n",
    "|publication_id|publication|\n",
    "|--------------|-----------|\n",
    "|          6487|     AA2 35|\n",
    "|          6488|   M/PRM 35|\n",
    "|          6489|   M/PRM 36|\n",
    "|          6490|   M/PRM 37|\n",
    "|          6491|      WI? 9|\n",
    "|          6492|      AVF 4|\n",
    "|          6493|      AVF 5|\n",
    "|          6494|     H2 251|\n",
    "|          6495|     H2 252|\n",
    "|          6496|      COC 1|\n",
    "\n",
    "\n",
    "#### The relationships (aka edges list) (section begins in line 19430 with a header in the form of `*Edgeslist`) lines 19431 to the end of the file. \n",
    "\n",
    "- The relationships information is in the following format: `2 6487 6488 6489 6490 6491 6492 6493 6494 6495 6496`. This represents a relationship between the character primary key (the first number) and the publication primary keys (all other numbers.). The relationships table should look like the following table sample after processing.\n",
    "\n",
    "|character_id|publication_id|\n",
    "|------------|--------------|\n",
    "|           1|          6487|\n",
    "|           2|          6488|\n",
    "|           2|          6489|\n",
    "|           2|          6490|\n",
    "|           2|          6491|\n",
    "|           2|          6492|\n",
    "|           2|          6493|\n",
    "|           2|          6494|\n",
    "|           2|          6495|\n",
    "|           2|          6496|\n",
    "\n",
    "#### After combining the relationships with the character and publication information, the analytics table table should look like the table sample below:\n",
    "\n",
    "|character_id|           character|publication_id|publication|\n",
    "|------------|--------------------|--------------|-----------|\n",
    "|           1|24-HOUR MAN/EMMANUEL|          6487|     AA2 35|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6488|   M/PRM 35|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6489|   M/PRM 36|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6490|   M/PRM 37|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6491|      WI? 9|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6492|      AVF 4|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6493|      AVF 5|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6494|     H2 251|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6495|     H2 252|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6496|      COC 1|\n",
    "\n",
    "#### The goal is to clean the data tables so that you can easily leverage SQL commands to join the tables together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all your Spark related environment variables, and pyspark using the `findspark.init()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your SparkSession. You are only going to create a `SparkSession`, not a `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/12 23:27:28 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/12 23:27:38 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.appName(\"lab-07-app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your SparkSession is active by running the `spark` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-30-61.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab-07-app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1fc877aad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data.\n",
    "\n",
    "Although the data is a flat file with the structure explained earlier, you will be working with this file using [Spark DataFrame API and Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "Load in the text file using [Generic load functions for SparkSQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources), which is located at `s3://bigdatateaching/marvel/porgat.txt`. This file is also located in Azure Blob at `wasbs://marvel@bigdatateaching.blob.core.windows.net/porgat.txt`. \n",
    "\n",
    "Create a DataFrame called `df_in`, which will have a single field, where each record is the full text per line in the original text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = spark.read.text('s3://bigdatateaching/marvel/porgat.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first few lines of `df_in`. Using the `.show()` method. What is the default field name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|*Vertices 19428 6486|\n",
      "|1 \"24-HOUR MAN/EM...|\n",
      "|2 \"3-D MAN/CHARLE...|\n",
      "|3 \"4-D MAN/MERCURIO\"|\n",
      "|         4 \"8-BALL/\"|\n",
      "|               5 \"A\"|\n",
      "|           6 \"A'YIN\"|\n",
      "|    7 \"ABBOTT, JACK\"|\n",
      "|         8 \"ABCISSA\"|\n",
      "|            9 \"ABEL\"|\n",
      "|10 \"ABOMINATION/E...|\n",
      "|11 \"ABOMINATION |...|\n",
      "|    12 \"ABOMINATRIX\"|\n",
      "|        13 \"ABRAXAS\"|\n",
      "|     14 \"ADAM 3,031\"|\n",
      "|        15 \"ABSALOM\"|\n",
      "|16 \"ABSORBING MAN...|\n",
      "|17 \"ABSORBING MAN...|\n",
      "|           18 \"ACBA\"|\n",
      "|19 \"ACHEBE, REVER...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in `df_in` using the `count()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema of your dataframe `df_in` using the method `.printSchema()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider where the data is going to end up when you run the `take` command on a DataFrame. Get the first few records of the `df_in` and save it into an object in your Python session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df_in.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the local Python object. How is it different than the DataFrame in the cluster? Read up on the [Pyspark Row object](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=row#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='*Vertices 19428 6486')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*Vertices 19428 6486'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is critical not to \"take\" too much data from your cluster to your local session. There is no hard and fast rule, but more than 10,000 rows could start to slow down your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the data\n",
    "\n",
    "Since the data is in text format, at the end of this section you will have three DataFrames: one for the characters, one for the publications, and one for the relationship between characters and publications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API and SparkSQL functions are in the [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions) library. You can import all of the functions or specific functions as needed.\n",
    "\n",
    "Start by importing the `monotonically_increasing_id` function from the `pyspark.sql.functions` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe called `df_in_idx` where you will add a new field called `idx` that uses the `monotonically_increasing_id` to add a unique incremental numeric index to each record that increases from 1 at the beginning of the data to N at the end of the data. You should read more about the function and understand how it works [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.monotonically_increasing_id.html). \n",
    "\n",
    "To make a new column, you must use the method `.withColumn([NEW COL NAME], [CODE TO MAKE COL])`. Read more about the method [here](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_idx = df_in.withColumn('idx', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your new dataframe, you will see the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|idx|\n",
      "+--------------------+---+\n",
      "|*Vertices 19428 6486|  0|\n",
      "|1 \"24-HOUR MAN/EM...|  1|\n",
      "|2 \"3-D MAN/CHARLE...|  2|\n",
      "|3 \"4-D MAN/MERCURIO\"|  3|\n",
      "|         4 \"8-BALL/\"|  4|\n",
      "|               5 \"A\"|  5|\n",
      "|           6 \"A'YIN\"|  6|\n",
      "|    7 \"ABBOTT, JACK\"|  7|\n",
      "|         8 \"ABCISSA\"|  8|\n",
      "|            9 \"ABEL\"|  9|\n",
      "|10 \"ABOMINATION/E...| 10|\n",
      "|11 \"ABOMINATION |...| 11|\n",
      "|    12 \"ABOMINATRIX\"| 12|\n",
      "|        13 \"ABRAXAS\"| 13|\n",
      "|     14 \"ADAM 3,031\"| 14|\n",
      "|        15 \"ABSALOM\"| 15|\n",
      "|16 \"ABSORBING MAN...| 16|\n",
      "|17 \"ABSORBING MAN...| 17|\n",
      "|           18 \"ACBA\"| 18|\n",
      "|19 \"ACHEBE, REVER...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in_idx.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the data file that the two headers are in lines 1 and 19430, and we want those lines (records) from the data. Filter the data using a regex to only include values rows that start with a `*`. The code is provided in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               value|idx|\n",
      "+--------------------+---+\n",
      "|*Vertices 19428 6486|  0|\n",
      "|          *Edgeslist|  1|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a regex search on the value column for rows that start with *\n",
    "df_in_idx.filter(df_in.value.rlike('^\\*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: after we make any change to our dataframe, we need to examine the results. This is accomplished using the `.show()` method as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a new dataframe called `df_idx_no_hdr` where using a sql query where you select all records but those with the header. But wait...\n",
    "\n",
    "Before you can run a SparkSQL query, you need to \"register\" a dataframe. This can be accomplished with the method `.createOrReplaceTempView([TABLE_NAME])`. Set up the \"register\" your dataframe `df_in_idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_idx.createOrReplaceTempView(\"df_in_idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the function `spark.sql([SQL STATEMENT])` to make the new dataframe `df_idx_no_hdr` that removes the rows with header info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_no_hdr = spark.sql(\"select * from df_in_idx\\\n",
    " where value rlike '^[0-9]';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the header rows were removed by counting the number of rows in the resulting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30518"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx_no_hdr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables\n",
    "\n",
    "Now you will create three separate dataframes: a `characters` one, a `publications` one, and a `relationships` one. You know the original line indices that partition the file, so use those. You have the `idx` field to use.\n",
    "\n",
    "You will need to subset the dataframe `df_idx_no_hdr` using the `.filter()` method. The `col([VAR NAME])` function is used within a dataframe method to refer to a variable using the variable name. After making your dataframes, check the number of rows and show the first few rows for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = df_idx_no_hdr.filter(col('idx')<6486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = df_idx_no_hdr.filter(col('idx')>=6486).filter(col('idx')<=19428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = df_idx_no_hdr.filter(col('idx')>19428)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Value from Data\n",
    "\n",
    "Register the characters and publications dataframes in order to run SQL commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.createOrReplaceTempView('characters')\n",
    "publications.createOrReplaceTempView('publications')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `regexp_extract` function within a SQL statement and a regular expression to create three fields from the whole line in the `character` and `publication` tables: \n",
    "\n",
    "1. the first field will be the integer (which is the node id)\n",
    "2. the second is the text **between** the double quotes\n",
    "3. the third is a string variable with the value \"character\" or \"publication\". \n",
    "\n",
    "For help with the regex, try going to https://regex101.com/ and paste in some of the lines to test the regex query.\n",
    "\n",
    "Create two new separate dataframes, one for characters and the other for publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_df = spark.sql(\"select regexp_extract(value, '^([0-9]*)\\') as character_id, regexp_extract(value, '\\\"(.*)(\\\")') as character, 'character' as type\\\n",
    "          from characters;\")\n",
    "publication_df = spark.sql(\"select regexp_extract(value, '^([0-9]*)\\') as publication_id, regexp_extract(value, '\\\"(.*)(\\\")') as publication, 'publication' as type\\\n",
    "          from publications;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look like the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+\n",
      "|character_id|           character|     type|\n",
      "+------------+--------------------+---------+\n",
      "|           1|24-HOUR MAN/EMMANUEL|character|\n",
      "|           2|3-D MAN/CHARLES CHAN|character|\n",
      "|           3|    4-D MAN/MERCURIO|character|\n",
      "|           4|             8-BALL/|character|\n",
      "|           5|                   A|character|\n",
      "+------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+\n",
      "|publication_id|publication|       type|\n",
      "+--------------+-----------+-----------+\n",
      "|          6486|       ZONE|publication|\n",
      "|          6487|     AA2 35|publication|\n",
      "|          6488|   M/PRM 35|publication|\n",
      "|          6489|   M/PRM 36|publication|\n",
      "|          6490|   M/PRM 37|publication|\n",
      "+--------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "publication_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack both the `character` and `publication` dataframes into a single dataframe called `nodes_df`, and cache it.\n",
    "\n",
    "Stacking dataframes by rows can be accomplished using the `.union()` method, which you can read about [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = char_df.union(publication_df).withColumnRenamed('character_id', 'id').withColumnRenamed('character', 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = nodes_df.withColumn('id', col('id').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, string: string, type: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will work on the `relationships` dataframe. Import the `split` and `explode` functions from the `pyspark.sql.functions` library. Review the following cells in preparation for the major chunk of code you will write. These cells are to show you examples of running pyspark commands with the split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               value|  idx|\n",
      "+--------------------+-----+\n",
      "|              1 6487|19430|\n",
      "|2 6488 6489 6490 ...|19431|\n",
      "|3 6497 6498 6499 ...|19432|\n",
      "|    4 6506 6507 6508|19433|\n",
      "|    5 6509 6510 6511|19434|\n",
      "|6 6512 6513 6514 ...|19435|\n",
      "|              7 6516|19436|\n",
      "|         8 6517 6518|19437|\n",
      "|         9 6519 6520|19438|\n",
      "|10 6521 6522 6523...|19439|\n",
      "|10 6536 6537 6538...|19440|\n",
      "|10 6551 6552 6553...|19441|\n",
      "|             11 6566|19442|\n",
      "|   12 6567 6568 6569|19443|\n",
      "|13 6570 6571 6572...|19444|\n",
      "|14 6574 6575 6576...|19445|\n",
      "|15 6578 6579 6580...|19446|\n",
      "|16 6582 6583 6584...|19447|\n",
      "|16 6597 6598 6599...|19448|\n",
      "|16 6612 6613 6614...|19449|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relationships.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = relationships.withColumn(\"newcol\", split(col(\"value\"), \" \")).cache() #convert a single text to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|               value|  idx|              newcol|\n",
      "+--------------------+-----+--------------------+\n",
      "|              1 6487|19430|           [1, 6487]|\n",
      "|2 6488 6489 6490 ...|19431|[2, 6488, 6489, 6...|\n",
      "|3 6497 6498 6499 ...|19432|[3, 6497, 6498, 6...|\n",
      "|    4 6506 6507 6508|19433|[4, 6506, 6507, 6...|\n",
      "|    5 6509 6510 6511|19434|[5, 6509, 6510, 6...|\n",
      "|6 6512 6513 6514 ...|19435|[6, 6512, 6513, 6...|\n",
      "|              7 6516|19436|           [7, 6516]|\n",
      "|         8 6517 6518|19437|     [8, 6517, 6518]|\n",
      "|         9 6519 6520|19438|     [9, 6519, 6520]|\n",
      "|10 6521 6522 6523...|19439|[10, 6521, 6522, ...|\n",
      "|10 6536 6537 6538...|19440|[10, 6536, 6537, ...|\n",
      "|10 6551 6552 6553...|19441|[10, 6551, 6552, ...|\n",
      "|             11 6566|19442|          [11, 6566]|\n",
      "|   12 6567 6568 6569|19443|[12, 6567, 6568, ...|\n",
      "|13 6570 6571 6572...|19444|[13, 6570, 6571, ...|\n",
      "|14 6574 6575 6576...|19445|[14, 6574, 6575, ...|\n",
      "|15 6578 6579 6580...|19446|[15, 6578, 6579, ...|\n",
      "|16 6582 6583 6584...|19447|[16, 6582, 6583, ...|\n",
      "|16 6597 6598 6599...|19448|[16, 6597, 6598, ...|\n",
      "|16 6612 6613 6614...|19449|[16, 6612, 6613, ...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "r2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using a chain of dataframe api methods, start with the value field in the `relationships` dataframe, where you will create a dataframe that takes the first value of the edge and puts it in the character field, and then \"explodes\" the other values in the publication column (1 row per character-publication combination).\n",
    "\n",
    "To make a new column, you must use the method `.withColumn([NEW COL NAME], [CODE TO MAKE COL])`. Read more about the method [here](https://sparkbyexamples.com/pyspark/pyspark-withcolumn/).\n",
    "\n",
    "There are six chained methods necessary:\n",
    "\n",
    "1. make a new column using the `split` function from column `value` as we just did above in the example code\n",
    "2. make a new column `character` by grabbing the first item from the list column you just made in step 1. Read about a [useful list column function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.getItem.html).\n",
    "3. make a new column `publication` by using the `explode` function to make each publication in a list as a separate row. Read more about the `explode` function [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html).\n",
    "4. Select only the `character` and `publication` columns\n",
    "5. Filter the dataframe to include only rows where the character and publication columns are not equal\n",
    "6. Cache your results\n",
    "7. Save the final dataframe as `edges_df`\n",
    "\n",
    "After each step 1-5 you should use a `.show(10, truncate = False)` method to examine the current state of your dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = relationships.withColumn(\"edge\", split(col(\"value\"), \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.withColumn(\"character\", temp_df['edge'].getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.withColumn(\"publication\", explode(temp_df['edge']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.select('character','publication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.filter(col('character') != col('publication'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = temp_df.withColumn('character', col('character').cast('int')).withColumn('publication', col('publication').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|character|publication|\n",
      "+---------+-----------+\n",
      "|1        |6487       |\n",
      "|2        |6488       |\n",
      "|2        |6489       |\n",
      "|2        |6490       |\n",
      "|2        |6491       |\n",
      "|2        |6492       |\n",
      "|2        |6493       |\n",
      "|2        |6494       |\n",
      "|2        |6495       |\n",
      "|2        |6496       |\n",
      "+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[character: int, publication: int]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register your `nodes_df` and `edges_df` as SQL views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.createOrReplaceTempView('nodes_df')\n",
    "edges_df.createOrReplaceTempView('edges_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an analytical dataset\n",
    "\n",
    "You will now create an analytical dataset using SparkSQL where you will join both tables (nodes_df and edge_df) so you have the data you need to run some analytics on the data.\n",
    "\n",
    "1. Start off by showing 10 rows from both datasets and get a count of rows. You can run the show and count commands in the same cell.\n",
    "2. Then print the schemas of both dataframes to confirm data type match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "| id|              string|     type|\n",
      "+---+--------------------+---------+\n",
      "|  1|24-HOUR MAN/EMMANUEL|character|\n",
      "|  2|3-D MAN/CHARLES CHAN|character|\n",
      "|  3|    4-D MAN/MERCURIO|character|\n",
      "|  4|             8-BALL/|character|\n",
      "|  5|                   A|character|\n",
      "|  6|               A'YIN|character|\n",
      "|  7|        ABBOTT, JACK|character|\n",
      "|  8|             ABCISSA|character|\n",
      "|  9|                ABEL|character|\n",
      "| 10|ABOMINATION/EMIL BLO|character|\n",
      "+---+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|character|publication|\n",
      "+---------+-----------+\n",
      "|        1|       6487|\n",
      "|        2|       6488|\n",
      "|        2|       6489|\n",
      "|        2|       6490|\n",
      "|        2|       6491|\n",
      "|        2|       6492|\n",
      "|        2|       6493|\n",
      "|        2|       6494|\n",
      "|        2|       6495|\n",
      "|        2|       6496|\n",
      "+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      " |-- type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- character: integer (nullable = true)\n",
      " |-- publication: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue with the data types! We need to cast the `edges_df` columns to long format. This can be accomplished by calling the cast method on a variable of a dataframe, such as `df.var1.cast('str')`. See more examples of type [casting in PySpark here](https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/). You can also use the `col()` function we used previously like `col('var1').cast('str')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = edges_df.withColumn('character', col('character').cast('long')).withColumn('publication', col('publication').cast('long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[character: bigint, publication: bigint]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges dataframe needs information about each node. You will merge the nodes data for the publications and for the characters. This can be accomplished using Spark SQL or using Spark DataFrame commands.\n",
    "\n",
    "Read about [joins in PySparkSQL here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html). If you want to use PySparkSQL, you have to use the `char_df` and `publication_df` instead of nodes_df.\n",
    "\n",
    "Make sure the node_id columns are renamed to `character_id` and `publication_id` using the method like `df.withColumnRenamed('var1','var2')`. Do the same for the name columns to `character` and `publication`.\n",
    "\n",
    "Drop the node_type columns using the method `df.drop('var1')`.\n",
    "\n",
    "The final dataset should look like:\n",
    "\n",
    "|character_id|           character|publication_id|publication|\n",
    "|------------|--------------------|--------------|-----------|\n",
    "|           1|24-HOUR MAN/EMMANUEL|          6487|     AA2 35|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6488|   M/PRM 35|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6489|   M/PRM 36|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6490|   M/PRM 37|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6491|      WI? 9|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6492|      AVF 4|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6493|      AVF 5|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6494|     H2 251|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6495|     H2 252|\n",
    "|           2|3-D MAN/CHARLES CHAN|          6496|      COC 1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = edges_df.join(nodes_df, edges_df['character']==nodes_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop('id', 'type').withColumnRenamed('character', 'character_id').withColumnRenamed('string', 'character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.join(nodes_df, temp_df['publication']==nodes_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop('id', 'type').withColumnRenamed('publication', 'publication_id').withColumnRenamed('string', 'publication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[character_id: bigint, publication_id: bigint, character: string, publication: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Analytics on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 publications with the highest number of characters present in them? Use Spark SQL to answer this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_pub_df = spark.sql('select count(character_id) as character_counts, publication \\\n",
    "            from df \\\n",
    "            group by publication \\\n",
    "            order by count(character_id) desc\\\n",
    "            limit 10;')\n",
    "# once you experiment and write the proper code then save to the dictionary key below\n",
    "dict_answers = {}\n",
    "dict_answers['q1'] = list((top_10_pub_df.toPandas())['publication'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 characters who appear in the most publications? Use dataframe methods to answer this question. You will need to use [groupby](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/), [agg](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html), and [sort](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/) functions.\n",
    "\n",
    "Example: \n",
    "\n",
    "```\n",
    "import pyspark.sql.functions as f\n",
    "(df\n",
    "  .groupby('grouping_variable')\n",
    "  .agg(  f.count(col('var1')).alias('var1_ct')  )\n",
    "  .sort(  col('var1_ct').desc()  )\n",
    "  .show(10)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "top_10_cha_df = df.groupby('character').agg( f.count(col('publication')).alias('publication_counts')).sort(col('publication_counts').desc()).take(10)\n",
    "# once you experiment and write the proper code then save to the dictionary key below\n",
    "dict_answers['q2'] = list(spark.createDataFrame(top_10_cha_df).toPandas()['character'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(str(dict_answers), fp = open('lab-solution.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP YOUR CLUSTER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
