{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Prep in PySpark\n",
    "\n",
    "In this workbook, you will read in the `trip` and `fare` files. You are welcome to use DataFrame and/or SparkSQL API as you desire as long as it produces the expected results.\n",
    "\n",
    "It is recommended to make small versions of the datasets for testing purposes so that you can trial and error faster. Run a command similar to `df_small = df.limit(10000)`\n",
    "\n",
    "Make sure you use your [PySparkSQL Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) to help you with the commands to complete the assignment.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Read in both datasets and conduct initial exploration. \n",
    "    - Determine the shape of the data, column names, data types\n",
    "    - Check for the number of missing values on the critical fields of the datasets - pickup_datetime, dropoff_datetime, passenger_count, trip_distance, fare_amount, medallion, hack_license, etc. There is a `count` command to use with columns in a dataframe and several ways to check for missing values. Try Googling some options!\n",
    "    - Check the counts for values of passenger_count and remove any outlier values (how many people fit into a taxi?). Justify your analytical decision. You can use the `.isin()` method to create boolean values from a column ([see here](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.isin.html)).\n",
    "\n",
    "2. Join both datasets such that you get a merged dataset with 21 unique fields. \n",
    "    - Which columns do you use to join the data? You need to determine how to match the schemas.\n",
    "    - What type of join should you use? Try at least two types of joins and report how much data is matched and is lost based on the join type. Justify which join you selected.\n",
    "\n",
    "3. Once you create the merged dataset, you need to convert fields to the following types, since all fields were read in as string:\n",
    "    * pickup_datetime and dropoff_datetime must be TIMESTAMP\n",
    "    * passenger_count and rate_code must be INT\n",
    "    * all other numeric fields must be FLOAT\n",
    "    * the remaining fields stay as STRING\n",
    "\n",
    "4. Create new variables to your dataset that will be important for understanding the data.\n",
    "    * Dummy variable for if pickup_datetime appears on a weekend or not\n",
    "    * Dummy variable for if pickup_datetime appears during weekday rush hour (6:00-9:59am, 2:00-5:59pm) or not\n",
    "    * Dummy variable for if the tip_amount is greater than 10% of the fare_amount or not\n",
    "\n",
    "5. Save this merged, converted, and transformed dataset to your own S3 bucket in parquet format.\n",
    "\n",
    "6. Report the average, median, Q25, and Q75 fare amount in a table conditioned on the dummy variables you created in step 4. Each table should look like the example below, though your numbers might not match exactly.\n",
    "\n",
    "\n",
    " |  weekdayrush_dummy  |mean_fare  |Q25|  median|   Q75|  num_rides|\n",
    "|----------------------|-----------|---|--------|------|-----------|\n",
    "|0               |True     |12.462  |6.5     |9.0  |14.0   |44848941|\n",
    "|1              |False     |12.312  |6.5     |9.5  |14.0  |128336134|\n",
    "       \n",
    "You are welcome to add as many cells as you need. Clearly indicate in your notebook each step so graders can confirm you have accomplished each task. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REQUIRED:** Start off by copying the taxi data from the public S3 bucket to your personal S3 bucket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-13 19:28:51,860 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[BLOCKSIZE], atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3://bigdatateaching/nyctaxi-2013/parquet], targetPath=s3://mc2582/taxi, filtersFile='null', blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, directWrite=false}, sourcePaths=[s3://bigdatateaching/nyctaxi-2013/parquet], targetPathExists=true, preserveRawXattrsfalse\n",
      "2022-10-13 19:28:52,181 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-20-83.ec2.internal/172.31.20.83:8032\n",
      "2022-10-13 19:28:52,400 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-20-83.ec2.internal/172.31.20.83:10200\n",
      "2022-10-13 19:28:53,528 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 371; dirCnt = 3\n",
      "2022-10-13 19:28:53,529 INFO tools.SimpleCopyListing: Build file listing completed.\n",
      "2022-10-13 19:28:53,530 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2022-10-13 19:28:53,530 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2022-10-13 19:28:53,632 INFO tools.DistCp: Number of paths in the copy list: 371\n",
      "2022-10-13 19:28:53,690 INFO tools.DistCp: Number of paths in the copy list: 371\n",
      "2022-10-13 19:28:53,707 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-20-83.ec2.internal/172.31.20.83:8032\n",
      "2022-10-13 19:28:53,708 INFO client.AHSProxy: Connecting to Application History server at ip-172-31-20-83.ec2.internal/172.31.20.83:10200\n",
      "2022-10-13 19:28:53,773 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1665679371880_0004\n",
      "2022-10-13 19:28:53,888 INFO mapreduce.JobSubmitter: number of splits:21\n",
      "2022-10-13 19:28:54,009 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1665679371880_0004\n",
      "2022-10-13 19:28:54,010 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-10-13 19:28:54,170 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-10-13 19:28:54,170 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-10-13 19:28:54,233 INFO impl.YarnClientImpl: Submitted application application_1665679371880_0004\n",
      "2022-10-13 19:28:54,268 INFO mapreduce.Job: The url to track the job: http://ip-172-31-20-83.ec2.internal:20888/proxy/application_1665679371880_0004/\n",
      "2022-10-13 19:28:54,268 INFO tools.DistCp: DistCp job-id: job_1665679371880_0004\n",
      "2022-10-13 19:28:54,268 INFO mapreduce.Job: Running job: job_1665679371880_0004\n",
      "2022-10-13 19:29:00,327 INFO mapreduce.Job: Job job_1665679371880_0004 running in uber mode : false\n",
      "2022-10-13 19:29:00,328 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-10-13 19:29:08,391 INFO mapreduce.Job:  map 5% reduce 0%\n",
      "2022-10-13 19:29:09,395 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2022-10-13 19:29:14,417 INFO mapreduce.Job:  map 29% reduce 0%\n",
      "2022-10-13 19:29:15,422 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "2022-10-13 19:29:16,426 INFO mapreduce.Job:  map 90% reduce 0%\n",
      "2022-10-13 19:29:17,431 INFO mapreduce.Job:  map 95% reduce 0%\n",
      "2022-10-13 19:29:19,438 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-10-13 19:29:21,450 INFO mapreduce.Job: Job job_1665679371880_0004 completed successfully\n",
      "2022-10-13 19:29:21,541 INFO mapreduce.Job: Counters: 42\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=5073116\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=120707\n",
      "\t\tHDFS: Number of bytes written=44410\n",
      "\t\tHDFS: Number of read operations=168\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=42\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=21\n",
      "\t\tOther local map tasks=21\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23174592\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=241402\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=241402\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=741586944\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=371\n",
      "\t\tMap output records=368\n",
      "\t\tInput split bytes=2856\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=6627\n",
      "\t\tCPU time spent (ms)=142760\n",
      "\t\tPhysical memory (bytes) snapshot=8858017792\n",
      "\t\tVirtual memory (bytes) snapshot=93589868544\n",
      "\t\tTotal committed heap usage (bytes)=8432123904\n",
      "\t\tPeak Map Physical memory (bytes)=551514112\n",
      "\t\tPeak Map Virtual memory (bytes)=4489412608\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=117851\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=44410\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=0\n",
      "\t\tBytes Skipped=6692038168\n",
      "\t\tDIR_COPY=3\n",
      "\t\tFiles Skipped=368\n"
     ]
    }
   ],
   "source": [
    "!hadoop distcp s3://bigdatateaching/nyctaxi-2013/parquet/ s3://mc2582/taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/14 03:38:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/14 03:38:16 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.appName(\"a6-part2-app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip = spark.read.parquet(\"s3://mc2582/taxi/trip/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = spark.read.parquet(\"s3://mc2582/taxi/fare/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (42 + 1) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173179759 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(trip.count(), len(trip.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================================> (44 + 1) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173179759 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(fare.count(), len(fare.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medallion',\n",
       " 'hack_license',\n",
       " 'vendor_id',\n",
       " 'rate_code',\n",
       " 'store_and_fwd_flag',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_time_in_secs',\n",
       " 'trip_distance',\n",
       " 'pickup_longitude',\n",
       " 'pickup_latitude',\n",
       " 'dropoff_longitude',\n",
       " 'dropoff_latitude']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medallion',\n",
       " 'hack_license',\n",
       " 'vendor_id',\n",
       " 'pickup_datetime',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'surcharge',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "trip.filter(col('pickup_datetime').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.filter(col('dropoff_datetime').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.filter(col('passenger_count').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.filter(col('trip_distance').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.filter(col('fare_amount').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.filter(col('medallion').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.filter(col('medallion').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare.filter(col('hack_license').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip.filter(col('hack_license').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_trip = trip.filter(col('passenger_count').isin(['1','2','3','4','5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Join the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fare.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df_test = valid_trip.join(fare, ['medallion', 'hack_license', 'vendor_id', 'pickup_datetime'], 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166409833"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_trip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166414859"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase records with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid_trip.join(fare, ['medallion', 'hack_license', 'vendor_id', 'pickup_datetime'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166414859"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't lose in number of records compared to valid_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Change data types\n",
    "This step can also be integrated into Step 2 merging if using PySparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid_df.withColumn('pickup_datetime', col('pickup_datetime').cast('timestamp'))\\\n",
    ".withColumn('dropoff_datetime', col('dropoff_datetime').cast('timestamp'))\\\n",
    ".withColumn('passenger_count', col('passenger_count').cast('int'))\\\n",
    ".withColumn('rate_code', col('rate_code').cast('int'))\\\n",
    ".withColumn('trip_time_in_secs', col('trip_time_in_secs').cast('float'))\\\n",
    ".withColumn('trip_distance', col('trip_distance').cast('float'))\\\n",
    ".withColumn('pickup_longitude', col('pickup_longitude').cast('float'))\\\n",
    ".withColumn('pickup_latitude', col('pickup_latitude').cast('float'))\\\n",
    ".withColumn('dropoff_longitude', col('dropoff_longitude').cast('float'))\\\n",
    ".withColumn('dropoff_latitude', col('dropoff_latitude').cast('float'))\\\n",
    ".withColumn('fare_amount', col('fare_amount').cast('float'))\\\n",
    ".withColumn('surcharge', col('surcharge').cast('float'))\\\n",
    ".withColumn('mta_tax', col('mta_tax').cast('float'))\\\n",
    ".withColumn('tip_amount', col('tip_amount').cast('float'))\\\n",
    ".withColumn('tolls_amount', col('tolls_amount').cast('float'))\\\n",
    ".withColumn('total_amount', col('total_amount').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[medallion: string, hack_license: string, vendor_id: string, pickup_datetime: timestamp, rate_code: int, store_and_fwd_flag: string, dropoff_datetime: timestamp, passenger_count: int, trip_time_in_secs: float, trip_distance: float, pickup_longitude: float, pickup_latitude: float, dropoff_longitude: float, dropoff_latitude: float, payment_type: string, fare_amount: float, surcharge: float, mta_tax: float, tip_amount: float, tolls_amount: float, total_amount: float]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create dummy variables\n",
    "\n",
    "- Dummy variable for if pickup_datetime appears on a weekend or not\n",
    "- Dummy variable for if pickup_datetime appears during weekday rush hour (6:00-9:59am, 2:00-5:59pm) or not\n",
    "- Dummy variable for if the tip_amount is greater than 10% of the fare_amount or not\n",
    "\n",
    "Reminder that using a small dataframe for testing will help you develop your analytics faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = valid_df.sample(True, 0.00001, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "def InsertIntoRow(row, keys, values):\n",
    "    temp_dict = row.asDict()\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        value = values[i]\n",
    "        temp_dict[key] = value\n",
    "    return Row(**temp_dict)\n",
    "def DateOnWeekend(date_time):\n",
    "    return date_time.weekday()>4\n",
    "def TimeInRushHour(date_time):\n",
    "    tmp_time = date_time.time()\n",
    "    rush_hour_1 = datetime.time(hour=6, minute=0, second=0)\n",
    "    rush_hour_2 = datetime.time(hour=9, minute=59, second=0)\n",
    "    rush_hour_3 = datetime.time(hour=13, minute=0, second=0)\n",
    "    rush_hour_4 = datetime.time(hour=17, minute=59, second=0)\n",
    "    return (tmp_time >= rush_hour_1 and tmp_time <= rush_hour_2) | (tmp_time >= rush_hour_3 and tmp_time <= rush_hour_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "final_df = valid_df.rdd.map(\\\n",
    "                           lambda row: InsertIntoRow(row, ['isOnWeekend', 'isInRushHour', 'isTipAmountGreat'],\\\n",
    "                                                     [DateOnWeekend(row.pickup_datetime), TimeInRushHour(row.pickup_datetime),\\\n",
    "                                                     (row.tip_amount)>(row.fare_amount*0.1)])).toDF()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.write.parquet('s3://mc2582/cleaned_taxi/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[medallion: string, hack_license: string, vendor_id: string, pickup_datetime: timestamp, rate_code: bigint, store_and_fwd_flag: string, dropoff_datetime: timestamp, passenger_count: bigint, trip_time_in_secs: double, trip_distance: double, pickup_longitude: double, pickup_latitude: double, dropoff_longitude: double, dropoff_latitude: double, payment_type: string, fare_amount: double, surcharge: double, mta_tax: double, tip_amount: double, tolls_amount: double, total_amount: double, isOnWeekend: boolean, isInRushHour: boolean, isTipAmountGreat: boolean]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_time_in_secs: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- isOnWeekend: boolean (nullable = true)\n",
      " |-- isInRushHour: boolean (nullable = true)\n",
      " |-- isTipAmountGreat: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "temp_weekend_mean = final_df.select('isOnWeekend', 'fare_amount').groupby('isOnWeekend').agg(f.mean(col('fare_amount'))).alias('mean_fare').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_weekend_quantiles = final_df.select('isOnWeekend', 'fare_amount').groupby('isOnWeekend')\\\n",
    ".agg(f.expr('percentile(fare_amount, array(0.25))')[0].alias('%25'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.50))')[0].alias('%50'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.75))')[0].alias('%75')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_rush_mean = final_df.select('isInRushHour', 'fare_amount').groupby('isInRushHour').agg(f.mean(col('fare_amount'))).alias('mean_fare').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_rush_quantiles = final_df.select('isInRushHour', 'fare_amount').groupby('isInRushHour')\\\n",
    ".agg(f.expr('percentile(fare_amount, array(0.25))')[0].alias('%25'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.50))')[0].alias('%50'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.75))')[0].alias('%75')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_greattip_mean = final_df.select('isTipAmountGreat', 'fare_amount').groupby('isTipAmountGreat').agg(f.mean(col('fare_amount'))).alias('mean_fare').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_greattip_quantiles = final_df.select('isTipAmountGreat', 'fare_amount').groupby('isTipAmountGreat')\\\n",
    ".agg(f.expr('percentile(fare_amount, array(0.25))')[0].alias('%25'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.50))')[0].alias('%50'),\\\n",
    "     f.expr('percentile(fare_amount, array(0.75))')[0].alias('%75')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(isOnWeekend=True, %25=6.5, %50=9.5, %75=14.0),\n",
       " Row(isOnWeekend=False, %25=6.5, %50=9.5, %75=14.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_weekend_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(isOnWeekend=True, %25=6.5, %50=9.5, %75=14.0),\n",
       " Row(isOnWeekend=False, %25=6.5, %50=9.5, %75=14.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_weekend_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_weekend = pd.DataFrame({'isOnWeekend':[True,False],\\\n",
    "                          'mean_fare':[temp_weekend_mean[0][1], temp_weekend_mean[1][1]],\\\n",
    "                          'Q25':[temp_weekend_quantiles[0][1], temp_weekend_quantiles[1][1]],\\\n",
    "                          'median':[temp_weekend_quantiles[0][2], temp_weekend_quantiles[1][2]],\\\n",
    "                          'Q75':[temp_weekend_quantiles[0][3], temp_weekend_quantiles[1][3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rushhour = pd.DataFrame({'isInRushHour':[True,False],\\\n",
    "                          'mean_fare':[temp_rush_mean[0][1], temp_rush_mean[1][1]],\\\n",
    "                          'Q25':[temp_rush_quantiles[0][1], temp_rush_quantiles[1][1]],\\\n",
    "                          'median':[temp_rush_quantiles[0][2], temp_rush_quantiles[1][2]],\\\n",
    "                          'Q75':[temp_rush_quantiles[0][3], temp_rush_quantiles[1][3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip = pd.DataFrame({'isTipAmountGreat':[True,False],\\\n",
    "                          'mean_fare':[temp_greattip_mean[0][1], temp_greattip_mean[1][1]],\\\n",
    "                          'Q25':[temp_greattip_quantiles[0][1], temp_greattip_quantiles[1][1]],\\\n",
    "                          'median':[temp_greattip_quantiles[0][2], temp_greattip_quantiles[1][2]],\\\n",
    "                          'Q75':[temp_greattip_quantiles[0][3], temp_greattip_quantiles[1][3]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({'weekend' : df_weekend.to_dict('records'),\n",
    "           'tip' : df_tip.to_dict('records'),\n",
    "           'rushhour' : df_rushhour.to_dict('records')\n",
    "          }, \n",
    "          fp = open('taxi-soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAKE SURE YOU STOP YOUR EMR CLUSTER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
