{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Basic RDD Operations\n",
    "\n",
    "This lab introduces you to working with Spark and with RDDs using a Jupyter Notebook and Pyspark as the way to interact with Spark. \n",
    "\n",
    "There are many methods that can be used with RDDs. See [this great cheat sheet by the DataCamp team](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf). A copy is also in this repository.\n",
    "\n",
    "Also, there is an [accompanying reference notebook](reference/reference-rdds.ipynb) that shows many RDD Transformations and Actions, which comes from the book [Learning Pyspark by Denny Lee and Thomas Drabasz](https://learning.oreilly.com/library/view/learning-pyspark/9781786463708/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/12 03:14:40 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "22/10/12 03:14:48 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-16-60.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD called `A` that reads the following text file: `s3://bigdatateaching/shakespeare/100-0.txt`, the complete works of William Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sc.\\\n",
    "    textFile(\n",
    "        's3://bigdatateaching/shakespeare/100-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type in `A` which shows you a pointer to the file in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3://bigdatateaching/shakespeare/100-0.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 elements of `A` by using the `take` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Project Gutenberg’s The Complete Works of William Shakespeare, by',\n",
       " 'William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, store the first 5 elements of `A` in a local Python object called `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "a = A.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of object is `a`? Remember, this is local object within your Python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the contents of `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Project Gutenberg’s The Complete Works of William Shakespeare, by',\n",
       " 'William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index into `a` using standard Python code. What is the second element in `a`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg’s The Complete Works of William Shakespeare, by'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try indexing into the RDD `A`. It won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19201/2665442193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "A[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements does `A` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147838"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked about keeping data in memory to reuse later. To do that, you use the `cache` method on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3://bigdatateaching/shakespeare/100-0.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python function checks wether the word \"Hamlet\" exists in a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasHamlet( s ):\n",
    "    return \"Hamlet\" in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new RDD called `b` that uses the Python `hasHamlet` function and returns only the RDD lines where Hamlet is in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = A.filter(lambda row: hasHamlet(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements does `b` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a few seconds, didn't it? Now try counting `A` again and see that it was much quicker than before (because it is cached.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147838"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `first` method to get the first element only of an RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using `first` with a value, like in the first 10 records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19201/4001899537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: first() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "A.first(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many RDD partitions does `A` RDD have? Use the `getNumPartitions` method to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sample records from an RDD using the `takeSample` method. Sample 10 records from `b` with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Enter Hamlet.',\n",
       " 'Bear Hamlet like a soldier to the stage,',\n",
       " 'Came this from Hamlet to her?',\n",
       " 'So please you, something touching the Lord Hamlet.',\n",
       " 'His fell to Hamlet. Now, sir, young Fortinbras,',\n",
       " 'Let not thy mother lose her prayers, Hamlet.',\n",
       " 'Thy loving father, Hamlet.',\n",
       " 'that young Hamlet was born,—he that is mad, and sent into England.',\n",
       " 'You go to seek the Lord Hamlet; there he is.',\n",
       " 'Letters, my lord, from Hamlet.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.takeSample(True, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will re-do one of the first assignment problems with the `quazyilx` dataset. First, create an RDD called `quazyilx` from the `s3://bigdatateaching/quazyilx/quazyilx1.txt` file (the ~5GB file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quazyilx = sc.textFile('s3://bigdatateaching/quazyilx/quazyilx1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many partitions the RDD has. This is analogous to the number of blocks the file is on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quazyilx.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and cache an RDD called `badrec` that uses a filter statement to find the bad records. Remember that each records is a whole line of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "badrec = quazyilx.filter(lambda row: len(row.split(':-1')) == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many bad records were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badrec.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get all the records for an RDD, then you need to use the `collect` method. Be careful, though, because if you use it with a large dataset, it could overflow your Python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000-01-28 03:07:44 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-02-21 19:21:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-03-01 17:31:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-04-29 03:37:34 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-07-08 21:27:33 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-08-15 19:21:29 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-09-09 19:25:47 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-10-02 15:06:39 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-11-14 06:08:56 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-11-15 01:31:47 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-12-12 00:52:10 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-03-30 00:24:35 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-04-06 13:31:09 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-06-27 07:10:50 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-07-11 00:05:17 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-07-17 19:57:09 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-10-03 14:21:16 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-10-17 18:12:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-10-24 11:33:13 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2001-12-21 20:57:35 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-01-23 11:33:45 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-02-08 06:07:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-03-06 21:50:24 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-03-17 22:44:06 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-04-20 06:50:59 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-05-27 13:35:56 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-06-26 15:59:53 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-09-19 01:07:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-10-02 15:16:11 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-10-13 03:50:40 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-12-03 11:42:40 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-12-20 08:37:02 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2002-12-29 14:42:32 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-01-17 16:34:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-01-28 15:55:41 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-02-16 13:41:24 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-04-16 16:20:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-06-09 18:59:25 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-06-17 05:27:16 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-06-25 14:28:43 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-07-23 04:17:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-08-04 10:38:44 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-08-06 01:05:34 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-08-08 20:02:16 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-09-22 09:31:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-09-26 08:09:16 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-09-28 19:13:59 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-10-06 04:21:53 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-10-18 15:16:28 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-10-24 04:14:23 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-11-05 21:42:48 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2003-11-07 14:34:51 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-01-19 16:23:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-06-17 05:33:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-06-23 16:13:24 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-07-04 17:33:45 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-07-10 10:57:38 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-08-01 21:11:57 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-09-11 23:52:00 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-10-04 08:04:48 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2004-10-18 02:09:21 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-01-10 00:38:06 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-01-12 18:42:51 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-02-02 07:50:15 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-02-12 15:21:20 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-02-27 02:24:26 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-03-20 10:22:11 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-03-31 22:36:24 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-06-16 19:27:41 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-07-29 06:40:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-08-04 17:53:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-08-13 06:41:30 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-09-01 15:16:54 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-09-16 20:27:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2005-12-20 08:32:02 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-02-23 02:17:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-03-05 11:54:34 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-03-05 17:27:11 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-04-17 14:01:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-05-31 17:46:37 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-06-17 04:40:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-07-13 20:07:51 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-08-23 12:03:08 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-11-11 23:05:33 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2006-12-07 12:25:13 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-01-02 10:12:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-01-04 06:25:35 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-03-12 01:19:50 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-03-25 06:45:12 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-04-23 04:57:30 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-05-20 00:24:12 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-09-19 09:00:26 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2007-09-21 03:18:56 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-01-25 15:03:49 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-02-19 21:55:53 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-02-22 12:03:56 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-03-01 16:15:39 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-03-05 08:53:40 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-03-20 14:06:45 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-06-17 14:23:12 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-06-26 12:16:35 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-06-30 09:38:48 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-07-22 20:23:39 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-07-27 17:33:25 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-08-03 20:28:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-09-21 07:02:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2008-12-11 02:56:15 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-01-30 14:27:04 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-02-05 14:08:13 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-05-21 00:03:08 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-05-23 16:46:10 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-06-04 10:05:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-10-14 01:25:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-10-17 23:47:14 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-10-18 21:03:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-11-08 21:06:57 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2009-11-23 17:18:50 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-01-24 17:21:21 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-01-29 21:02:17 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-03-09 14:51:37 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-03-09 17:19:04 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-05-01 08:14:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-05-24 15:42:50 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-06-11 20:09:55 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-06-22 20:40:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-08-10 10:36:43 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2010-10-26 16:20:42 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-03-13 12:16:09 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-04-01 20:25:14 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-05-21 10:26:33 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-07-12 00:27:38 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-07-15 02:33:57 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-10-17 23:07:28 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-11-24 22:49:40 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2011-12-30 18:37:11 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-01-29 19:10:17 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-03-16 12:49:55 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-05-14 05:32:43 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-05-22 06:22:11 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-10-02 22:10:43 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-10-28 17:44:42 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-11-12 07:28:10 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2012-12-11 06:06:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2013-04-10 16:30:13 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2013-06-27 14:06:12 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2013-08-14 11:35:20 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2013-09-28 11:14:55 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2013-12-29 10:27:24 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-02-06 20:57:02 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-02-15 13:50:06 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-02-22 18:41:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-04-04 13:42:50 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-05-07 19:17:08 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-07-05 11:28:25 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-11-16 16:55:00 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2014-12-17 01:45:04 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-02-05 07:20:04 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-02-25 11:31:58 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-03-15 19:44:30 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-04-11 14:06:09 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-04-14 14:46:29 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-04-24 02:25:46 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-07-08 18:28:38 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-07-24 17:23:31 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-08-09 13:12:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-08-14 00:18:48 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-09-17 04:33:33 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-09-22 20:06:27 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-10-09 14:29:37 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-12-08 22:37:16 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2015-12-27 09:36:32 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-03-30 21:16:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-05-04 22:54:45 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-05-17 18:22:18 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-07-31 23:10:51 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-08-30 02:30:42 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-09-01 14:46:43 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-09-13 19:23:14 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-11-08 08:58:20 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-11-13 01:50:02 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-11-25 18:08:01 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-11-27 19:10:29 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2016-12-19 11:35:39 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-01-09 16:41:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-01-28 15:21:54 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-02-24 04:03:17 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-02-26 09:34:58 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-03-17 04:53:34 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-04-08 02:26:10 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2017-04-21 04:57:10 fnard:-1 fnok:-1 cark:-1 gnuck:-1']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badrec.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 elements of bad_rec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000-01-28 03:07:44 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-02-21 19:21:07 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-03-01 17:31:22 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-04-29 03:37:34 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-07-08 21:27:33 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-08-15 19:21:29 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-09-09 19:25:47 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-10-02 15:06:39 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-11-14 06:08:56 fnard:-1 fnok:-1 cark:-1 gnuck:-1',\n",
       " '2000-11-15 01:31:47 fnard:-1 fnok:-1 cark:-1 gnuck:-1']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badrec.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type is `bad_rec`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(badrec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will work the ForensicsWiki logs dataset and use RDD methods to do the same analysis we did in previous homeworks.\n",
    "\n",
    "First, create an RDD called `forensicswiki` pointing to the ForensicsWiki dataset at `s3://bigdatateaching/forensicswiki/2012_logs.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "forensicswiki = sc.textFile('s3://bigdatateaching/forensicswiki/2012_logs.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells have Python code that will be run on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "date_re = re.compile(\"(\\d\\d/[a-zA-Z]+/\\d\\d\\d\\d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(line):\n",
    "    m = date_re.search(line)\n",
    "    if m:\n",
    "        d = datetime.datetime.strptime(m.group(1),\"%d/%b/%Y\")\n",
    "        return \"{:04}-{:02}\".format(d.year,d.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new RDD called `dates` that runs the `extract` function on every element in the `forensicswiki` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = forensicswiki.map(lambda row: extract(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `dates` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2012-01'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15949554"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the `dates` dataset by month, effectively conducting the **reducer** step. This can be accomplished by `dates.countByKey()` or using a manual reduction step like:\n",
    "\n",
    "```\n",
    "from operator import add\n",
    "add_by_date = dates.reduceByKey(add)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "month_count = dates.map(lambda row: (row, 1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_count_result = sorted(month_count.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2012-01', 1544100),\n",
       " ('2012-02', 1325030),\n",
       " ('2012-03', 1274061),\n",
       " ('2012-04', 1016456),\n",
       " ('2012-05', 1173380),\n",
       " ('2012-06', 1300250),\n",
       " ('2012-07', 1287187),\n",
       " ('2012-08', 1450426),\n",
       " ('2012-09', 1284945),\n",
       " ('2012-10', 1498895),\n",
       " ('2012-11', 1397343),\n",
       " ('2012-12', 1396198),\n",
       " ('2013-01', 1283)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_count_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save to your git repo as soln.json:**\n",
    "1. the first 100 rows of the `dates` dataset. This involves the `take` command.\n",
    "2. the first 10 month key-value pairs in the form [('2012-10', 1234567), ...] ordered by date descending (earliest date first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_first100_result = dates.take(100)\n",
    "first10_months = month_count_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump({'dates_df' : df_first100_result,\n",
    "           'first10' : first10_months},\n",
    "          fp = open('soln.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Git add, commit, and push all your files to GitHub!! You can use the built-in Git module in Jupyter Lab!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you close the Jupyter Notebook, it is best to close the connection to the Spark cluster. If you don't you may have an \"orphan\" connection that is eating up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
