{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment - Single Machine Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be investigating the Enron email corpus from the 2002 Federal Energy Regulatory Commission (FERB) investigation.  This corpus of emails is still used to train AI on topics related to corporate communication, as this is one of the few public datasets available. Though there are AI ethics questions if the data is used for language purposes without proper precautions. [Read this article](https://qz.com/work/1546565/the-emails-that-brought-down-enron-still-shape-our-daily-lives/) to learn more about the dataset and its implications for AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off by choosing the `ml.m5.xlarge` instance as your kernel. Then start conducting the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Download dataset and explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The research dataset is located here, maintained by Carnegie Mellon University, https://www.cs.cmu.edu/~enron/. Though due to its size we have created a smaller version on S3. Follow these steps to get the file:\n",
    "\n",
    "1. Copy the file from the class's public S3 bucket into your own S3 bucket. You must change the bucket name to yours with your net id. `aws s3 cp s3://bigdatateaching/enron/maildir.zip s3://[NET_ID]-labdata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Copy the file from your own S3 bucket to SageMaker. Use the boto3 package for this. This will place the `maildir` directory from the zip file into the tmp folder of your EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "my_bucket = 'mc2582-labdata'\n",
    "my_file = 'maildir.zip'\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "s3client.download_file(my_bucket, my_file, '/tmp/maildir.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the following command in a new cell to unzip the directory. This could take a while, there are almost 80,000 files!! We are explicitly saving to the /tmp file share because it is part of your EC2. If you use your git repository to store the maildir zip or unpacked files, we would be here a while...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /tmp; unzip -q /tmp/maildir.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bullets below represent the file structure of interest from the Enron dataset. To confirm that you have unzipped and placed the data in the correct location, click on the links of files, e.g. `1_`, `2_`, etc., which will open the text files for you to explore. Notice that these are all raw text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [maildir](./maildir)\n",
    " * [allen-p](./maildir/allen-p)\n",
    "   * [inbox](./maildir/allen-p/inbox)\n",
    "       * [1_](./maildir/allen-p/inbox/1_)\n",
    "       * [2_](./maildir/allen-p/inbox/2_)\n",
    "       * [3_](./maildir/allen-p/inbox/3_)\n",
    "       ...\n",
    "   * [sent_items](./maildir/allen-p/sent_items)\n",
    "       * [1_](./maildir/allen-p/sent_items/1_)\n",
    "       * [2_](./maildir/allen-p/sent_items/2_)\n",
    "       * [3_](./maildir/allen-p/sent_items/3_)\n",
    "       * etc...\n",
    "   * etc...\n",
    " * [arnold-j](./maildir/arnold-j)\n",
    "   * [inbox](./maildir/arnold-j/inbox)\n",
    "       * [1_](./maildir/arnold-j/inbox/1_)\n",
    "       * [2_](./maildir/arnold-j/inbox/2_)\n",
    "       * [3_](./maildir/arnold-j/inbox/3_)\n",
    "       * etc ...\n",
    "   * [sent_items](./maildir/arnold-j/sent_items)\n",
    "       * [1_](./maildir/arnold-j/sent_items/1_)\n",
    "       * [2_](./maildir/arnold-j/sent_items/2_)\n",
    "       * [3_](./maildir/arnold-j/sent_items/3_)\n",
    "       * etc...\n",
    "   * etc...\n",
    " * [arora-h](./maildir/arora-h)\n",
    "   * [inbox](./maildir/arora-h/inbox)\n",
    "       * [1_](./maildir/arora-h/inbox/1_)\n",
    "       * [2_](./maildir/arora-h/inbox/2_)\n",
    "       * [3_](./maildir/arora-h/inbox/3_)\n",
    "       * etc...\n",
    "   * [sent_items](./maildir/arora-h/sent_items)\n",
    "       * [1_](./maildir/arora-h/sent_items/1_)\n",
    "       * [2_](./maildir/arora-h/sent_items/2_)\n",
    "       * [3_](./maildir/arora-h/sent_items/3_)\n",
    "       * etc...\n",
    "   * etc...\n",
    " * etc... etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1: Collect all of the file paths of emails in the maildir (2 points)\n",
    "\n",
    "The output object will be called `list_emails` should contain 79,429 items and look similar to the following:\n",
    "```\n",
    "['/tmp/maildir/allen-p/inbox/10_',\n",
    " '/tmp/maildir/allen-p/inbox/11_',\n",
    " '/tmp/maildir/allen-p/inbox/12_',\n",
    " '/tmp/maildir/allen-p/inbox/13_',\n",
    " '/tmp/maildir/allen-p/inbox/14_',\n",
    " '/tmp/maildir/allen-p/inbox/15_',\n",
    " '/tmp/maildir/allen-p/inbox/16_',\n",
    " '/tmp/maildir/allen-p/inbox/17_',\n",
    " '/tmp/maildir/allen-p/inbox/18_',\n",
    " '/tmp/maildir/allen-p/inbox/19_',\n",
    " ...\n",
    " ]\n",
    "```\n",
    "\n",
    "The broad strokes to creating this list goes as follows:\n",
    "\n",
    "- use the command `os.listdir('/tmp/maildir')` to see the folders in the mail directory\n",
    "- then use the command `os.listdir('/tmp/maildir/allen-p')` to see the folder in Allen P's folder\n",
    "- then use the command `os.listdir('/tmp/maildir/allen-p/inbox')` to see the contents in Allen P's inbox and collect the full file paths\n",
    "- you need to also collect all the files in both the inbox and sent_items folders for Allen P, do not track emails from the other folders\n",
    "- next move onto Arnold J!\n",
    "- next move onto Arora H!\n",
    "- ....\n",
    "\n",
    "Light bulbs should start going off in your head at this point about ways to solve this problem. This is a repeated process that needs to happen many many times. A for loop is the simplest way to solve a repetitive task. **Use for loops** to collect all 79,429 file paths into a single list.\n",
    "\n",
    "**Hint:** look up `os.path.join()` or f-strings if you are having trouble creating the appropriate argument for the `os.listdir()` command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s) produce the list of emails as directed and save the result to the object `list_email_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_email_paths = []\n",
    "for member in os.listdir('/tmp/maildir'):\n",
    "    if os.path.exists('/tmp/maildir/'+member+'/inbox'):\n",
    "        for number in os.listdir('/tmp/maildir/'+member+'/inbox'):\n",
    "            if number[-1]=='_':\n",
    "                list_email_paths.append('/tmp/maildir/'+member+'/inbox/'+number)\n",
    "    if os.path.exists('/tmp/maildir/'+member+'/sent_items'):\n",
    "        for number in os.listdir('/tmp/maildir/'+member+'/sent_items'):\n",
    "            if number[-1]=='_':\n",
    "                list_email_paths.append('/tmp/maildir/'+member+'/sent_items/'+number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s) print the first 10 items of the list as well as the length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/maildir/allen-p/inbox/10_\n",
      "/tmp/maildir/allen-p/inbox/11_\n",
      "/tmp/maildir/allen-p/inbox/12_\n",
      "/tmp/maildir/allen-p/inbox/13_\n",
      "/tmp/maildir/allen-p/inbox/14_\n",
      "/tmp/maildir/allen-p/inbox/15_\n",
      "/tmp/maildir/allen-p/inbox/16_\n",
      "/tmp/maildir/allen-p/inbox/17_\n",
      "/tmp/maildir/allen-p/inbox/18_\n",
      "/tmp/maildir/allen-p/inbox/19_\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(list_email_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79429"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_email_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Read in one text email and parse out relevant information using a function (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get to parallelizing, you need to build the function that will execute on every email in your list. The objective of this task is to build the function called `email_process()` which will take as an input the path of an email and output the following information in a dictionary:\n",
    "\n",
    "* Email of the sender\n",
    "* Email(s) of the recipient\n",
    "* Email timestamp\n",
    "* Email subject\n",
    "* Email body\n",
    "\n",
    "An example output from the function on the first email in Allen P's inbox looks like the following:\n",
    "\n",
    "```\n",
    "{'from': 'heather.dunton@enron.com',\n",
    " 'to': 'k..allen@enron.com',\n",
    " 'timestamp': 'Fri, 7 Dec 2001 10:06:42 -0800 (PST)'\n",
    " 'subject': 'RE: West Position',\n",
    " 'body': ' \\nPlease let me know if you still need Curve Shift.\\n\\nThanks,\\nHeather\\n -----Original Message-----\\nFrom: \\tAllen, Phillip K.  \\nSent:\\tFriday, December 07, 2001 5:14 AM\\nTo:\\tDunton, Heather\\nSubject:\\tRE: West Position\\n\\nHeather,\\n\\nDid you attach the file to this email?\\n\\n -----Original Message-----\\nFrom: \\tDunton, Heather  \\nSent:\\tWednesday, December 05, 2001 1:43 PM\\nTo:\\tAllen, Phillip K.; Belden, Tim\\nSubject:\\tFW: West Position\\n\\nAttached is the Delta position for 1/16, 1/30, 6/19, 7/13, 9/21\\n\\n\\n -----Original Message-----\\nFrom: \\tAllen, Phillip K.  \\nSent:\\tWednesday, December 05, 2001 6:41 AM\\nTo:\\tDunton, Heather\\nSubject:\\tRE: West Position\\n\\nHeather,\\n\\nThis is exactly what we need.  Would it possible to add the prior day for each of the dates below to the pivot table.  In order to validate the curve shift on the dates below we also need the prior days ending positions.\\n\\nThank you,\\n\\nPhillip Allen\\n\\n -----Original Message-----\\nFrom: \\tDunton, Heather  \\nSent:\\tTuesday, December 04, 2001 3:12 PM\\nTo:\\tBelden, Tim; Allen, Phillip K.\\nCc:\\tDriscoll, Michael M.\\nSubject:\\tWest Position\\n\\n\\nAttached is the Delta position for 1/18, 1/31, 6/20, 7/16, 9/24\\n\\n\\n\\n << File: west_delta_pos.xls >> \\n\\nLet me know if you have any questions.\\n\\n\\nHeather'\n",
    " }\n",
    "```\n",
    "\n",
    "To process the raw text file, we recommend that you use the `email.parser.Parser()` function. Read documentation on it [here](https://docs.python.org/3/library/email.parser.html).\n",
    "\n",
    "The to field will need some cleaning. Remove any carriage returns, tabs, or spaces from the to field. \n",
    "\n",
    "It's OK that the email body is really messy right now. A future task is to clean it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s) produce the function `email_process()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "def email_process(path):\n",
    "    try:\n",
    "        ema = email.parser.Parser().parse(open(path, encoding = 'utf-8', errors = 'ignore'))\n",
    "        result = {'from': ema['From'], 'to': ema['To'], 'timestamp': ema['Date'], 'subject': ema['Subject'], 'body': ema.get_payload()}\n",
    "        return result\n",
    "    except:\n",
    "        print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, save a new object called `email_1` as the output for the file `maildir/allen-p/inbox/1_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_1 = email_process('/tmp/maildir/allen-p/inbox/1_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Read in all emails using multiprocessing (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to implement multiprocessing on a list of data:\n",
    "\n",
    "1. **The vanilla approach to parallelization** involves getting a list of the items to process, then using the multiple worker processes to complete all the items in parallel like queue system. The list gets processes by workers until there are no items left.\n",
    "2. **The splitting approach to parallelization** involves getting a list of items to process, splitting that list into a number of sublists, then split each sublist to a worker. Each worker then processes the sublist in serial.\n",
    "\n",
    "For more background on these two ideas, check out this useful [medium post](https://medium.com/idealo-tech-blog/parallelisation-in-python-an-alternative-approach-b2749b49a1e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are using a Windows machine (not in this lab)**, you will be unable to save your function in a cell and then run with multiprocesing. Instead, save two scripts `mail_functions_vanilla.py` and `mail_functions_split.py` with the functions needed for each approach. Import the module that you will execute in parallel by calling `import mail_functions_vanilla` and then calling a function within that module, just like you would any other module in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `time` module to track the time taken for each approach. Note that you may have files that are corrupted and you will have to figure out how to handle them gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s), implement the vanilla approach to parallelization to process all emails using `pool.map()`. Save the output list to an object called `out_vanilla`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 13.539299011230469\n",
      "time to set up pool 0.08559441566467285\n",
      "time to multiprocess 13.45361876487732\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "t1 = time.time()\n",
    "out_vanilla = pool.map(email_process , list_email_paths)\n",
    "\n",
    "t2 = time.time()\n",
    "pool.close()\n",
    "end = time.time()\n",
    "\n",
    "print('total time',end - start)\n",
    "print('time to set up pool',t1 - start)\n",
    "print('time to multiprocess',t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s), implement the splitting approach to parallelization to process all emails using `pool.map()`. Save the output list to an object called `out_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 14.073877573013306\n",
      "time to set up pool 0.14962458610534668\n",
      "time to multiprocess 13.919296503067017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def actual_task(sublist_of_paths):\n",
    "    sublist_results = []\n",
    "    for path in sublist_of_paths:\n",
    "        sublist_results.append(email_process(path))\n",
    "    return sublist_results\n",
    "\n",
    "def splitter(list_ids, NUM_WORKERS):\n",
    "    list_list_ids = []\n",
    "    for i in np.array_split(list_ids, NUM_WORKERS): \n",
    "        list_list_ids.append(list(i))\n",
    "    return list_list_ids\n",
    "\n",
    "start = time.time()\n",
    "ids = splitter(list_email_paths, mp.cpu_count())\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count()) \n",
    "t1 = time.time()\n",
    "res_list = pool.map(actual_task, ids)\n",
    "t2 = time.time()\n",
    "pool.close()\n",
    "out_split = [item for sublist in res_list for item in sublist]\n",
    "end = time.time()\n",
    "\n",
    "print('total time', end - start)\n",
    "print('time to set up pool',t1 - start)\n",
    "print('time to multiprocess',t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s), confirm that the results from each method are the same. If they are not, then figure out how to make them the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vanilla == out_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell(s), convert the list `out_split` to a Pandas DataFrame and save it as `df_emails`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_emails = pd.DataFrame(out_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Find the most frequent emailers in the dataset (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know which people sent or received the most emails in the dataset. Use your Pandas dataframe fields `to` and `from` to count the number of emails where an email address appears in the data.\n",
    "\n",
    "**You must use a dictionary** to keep track of the connections between email addresses. This will involve looping through each row and countering the dictionary up for each email instance found. For example, if we processed rows of the data and found that abc@enron.com sent one email to def@enron.com and ghi@enron.com, another sent email to jkl@enron.com as well as received an email from ghi@enron.com. This would result in a dictionary like so:\n",
    "\n",
    "```\n",
    "{\n",
    " 'abc@enron.com' : 3,\n",
    " 'ghi@enron.com' : 2,\n",
    " 'def@enron.com' : 1,\n",
    " 'jkl@enron.com' : 1\n",
    "}\n",
    "```\n",
    "\n",
    "Note that we are counting the email address whether it is the recipient or the sender of the email. It is certainly possible that both sides of the email conversation appears in the dataset, but it is OK to count that twice. The `To` field can contain multiple emails. You have to parse and count each email recipient separately!\n",
    "\n",
    "This process might require a lot of computing power to process every email! You must use `pool.map()` or another `multiprocessing` module function for the mapping part of the problem. The output will be a list of dictionaries like the example above. You will write a function to execute on a small chunk of the dataframe. After you have the list of dictionaries from each chunk, reduce them into a single dictionary.\n",
    "\n",
    "**BONUS:** 1 point - implement the same map and reduce operations leveraging pandas vectorized operations and groupby-summarize functions.\n",
    "\n",
    "\n",
    "**Hint:** break your dataframe into a list of smaller dataframes, and use that list to pass through `pool.map()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell(s) to answer this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_join(x):\n",
    "    if x[1]!=None:\n",
    "        return (x[0]+', '+x[1])\n",
    "    else:\n",
    "        return x[0]\n",
    "merged_list = df_emails[['from','to']].apply(row_join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_recipients(email_str):\n",
    "    temp_Counter = Counter([word.strip() for word in email_str.split(',')])\n",
    "    return {key: value for key, value in temp_Counter.items()}\n",
    "temp = count_recipients(merged_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_merge(dict_a, dict_b):\n",
    "    for key in dict_b:\n",
    "        if key in dict_a:\n",
    "            dict_a[key] = dict_a[key] + dict_b[key]\n",
    "        else:\n",
    "            dict_a[key] = dict_b[key]\n",
    "    return dict_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "result_dictionaries = pool.map(count_recipients , merged_list)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "final_dict = reduce(dictionary_merge, result_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(final_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, save a dictionary with only the 20 email addresses with the most emails to the object `dict_top_addresses_sent`\n",
    "\n",
    "**Hint:** The top email address should be `'jeff.dasovich@enron.com'` with 2,624 emails (though a slightly different number if OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_top_addresses_sent = {}\n",
    "for key, value in sorted_dict.items():\n",
    "    if len(dict_top_addresses_sent) < 20:\n",
    "        dict_top_addresses_sent[key]=value\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jeff.dasovich@enron.com': 2624, 'd..steffes@enron.com': 2402, 'louise.kitchen@enron.com': 2248, 'chris.germany@enron.com': 2136, 'gerald.nemec@enron.com': 2115, 'no.address@enron.com': 2054, 'sara.shackleton@enron.com': 1989, 'kimberly.watson@enron.com': 1883, 'm..presto@enron.com': 1743, 'matthew.lenhart@enron.com': 1670, 'marie.heard@enron.com': 1670, 'barry.tycholiz@enron.com': 1644, 'john.lavorato@enron.com': 1578, 'pete.davis@enron.com': 1540, 'rick.buy@enron.com': 1539, 'tana.jones@enron.com': 1538, 'j.kaminski@enron.com': 1534, 'john.arnold@enron.com': 1521, 'sally.beck@enron.com': 1518, 'kam.keiser@enron.com': 1469}\n"
     ]
    }
   ],
   "source": [
    "print(dict_top_addresses_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Task: Run the following cell so your outputs can be checked for accuracy - this is a requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "grading_dict = {'len_paths' : len(list_email_paths),\n",
    " 'email_1' : str(email_1),\n",
    " 'vanilla_split_match' : str(out_vanilla == out_split),\n",
    " 'df_emails' : df_emails.head(10).to_string(),\n",
    " 'dict_top_email_addresses' : str(dict_top_addresses_sent)\n",
    " }\n",
    "\n",
    "json.dump(grading_dict, fp = open('soln.json','w'))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
